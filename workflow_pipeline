
---

# ğŸ“„ INFERENCE.md (Project Report & Inference)

```markdown
# IMDB Sentiment Model â€” Inference & Project Report

This document explains **what was done**, **what we achieved**, and **how inference works** in this project.

---

## ğŸŒ± What we did
1. **Environment setup**: Installed Hugging Face `transformers`, `datasets`, `evaluate`, etc.
2. **Dataset preparation**: Cleaned IMDB reviews (CSV: `review`, `sentiment`).
3. **Preprocessing**: Tokenized reviews into input IDs with `distilbert-base-uncased`.
4. **Training**: Fine-tuned DistilBERT with Hugging Face `Trainer`.
   - 2000 train / 1000 test (subset) for smoke test.
   - 1 epoch, learning rate `2e-5`, batch size `8`.
5. **Evaluation**:
   - Accuracy ~83.6%
   - F1 (weighted) ~83.5%
   - Confusion matrix: good balance, slightly more false positives.
6. **Inference**:
   - Built a pipeline for sentiment classification.
   - Mapped `LABEL_0 â†’ Negative`, `LABEL_1 â†’ Positive`.
   - Logged every inference to `predictions_log.csv`.
7. **Governance**:
   - Metadata (`metadata.json`) with training arguments.
   - Eval reports stored (`eval_report.txt`).
   - Prediction logs (`predictions_log.csv`) for audit.

---

## ğŸ¯ What we achieved
- A functional **sentiment classifier** with >83% accuracy.
- End-to-end workflow: dataset â†’ training â†’ evaluation â†’ inference â†’ logging.
- Governance and reproducibility: every run is tracked and auditable.
- Ready-to-scale pipeline (can handle larger datasets and more epochs).

---

## âš¡ Inference

### Client View (simple, non-technical)
- Input: a movie review text.  
- Output: **â€œPositiveâ€** or **â€œNegativeâ€** with a confidence score.  
- Example:
  - *â€œI really loved this movie, the acting was fantastic!â€* â†’ Positive (0.93)
  - *â€œThis film was boring and a complete waste of time.â€* â†’ Negative (0.91)

### Technical View
- Inference pipeline uses Hugging Face `pipeline("text-classification", model=..., tokenizer=..., top_k=None)`.
- Model = DistilBERT with 2-class classification head (fine-tuned).
- Predictions are logits â†’ softmax â†’ label probabilities.
- Predictions saved with timestamp and confidence into `predictions_log.csv`.

---

## ğŸ”® Next Steps
- Full training (25k train / 25k test, 2â€“3 epochs).
- Hyperparameter tuning (LR, batch size, max_length).
- Error analysis (false positives/negatives inspection).
- Optional: wandb integration, Gradio demo.
