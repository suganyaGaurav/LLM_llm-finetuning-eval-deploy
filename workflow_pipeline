
---

# 📄 INFERENCE.md (Project Report & Inference)

```markdown
# IMDB Sentiment Model — Inference & Project Report

This document explains **what was done**, **what we achieved**, and **how inference works** in this project.

---

## 🌱 What we did
1. **Environment setup**: Installed Hugging Face `transformers`, `datasets`, `evaluate`, etc.
2. **Dataset preparation**: Cleaned IMDB reviews (CSV: `review`, `sentiment`).
3. **Preprocessing**: Tokenized reviews into input IDs with `distilbert-base-uncased`.
4. **Training**: Fine-tuned DistilBERT with Hugging Face `Trainer`.
   - 2000 train / 1000 test (subset) for smoke test.
   - 1 epoch, learning rate `2e-5`, batch size `8`.
5. **Evaluation**:
   - Accuracy ~83.6%
   - F1 (weighted) ~83.5%
   - Confusion matrix: good balance, slightly more false positives.
6. **Inference**:
   - Built a pipeline for sentiment classification.
   - Mapped `LABEL_0 → Negative`, `LABEL_1 → Positive`.
   - Logged every inference to `predictions_log.csv`.
7. **Governance**:
   - Metadata (`metadata.json`) with training arguments.
   - Eval reports stored (`eval_report.txt`).
   - Prediction logs (`predictions_log.csv`) for audit.

---

## 🎯 What we achieved
- A functional **sentiment classifier** with >83% accuracy.
- End-to-end workflow: dataset → training → evaluation → inference → logging.
- Governance and reproducibility: every run is tracked and auditable.
- Ready-to-scale pipeline (can handle larger datasets and more epochs).

---

## ⚡ Inference

### Client View (simple, non-technical)
- Input: a movie review text.  
- Output: **“Positive”** or **“Negative”** with a confidence score.  
- Example:
  - *“I really loved this movie, the acting was fantastic!”* → Positive (0.93)
  - *“This film was boring and a complete waste of time.”* → Negative (0.91)

### Technical View
- Inference pipeline uses Hugging Face `pipeline("text-classification", model=..., tokenizer=..., top_k=None)`.
- Model = DistilBERT with 2-class classification head (fine-tuned).
- Predictions are logits → softmax → label probabilities.
- Predictions saved with timestamp and confidence into `predictions_log.csv`.

---

## 🔮 Next Steps
- Full training (25k train / 25k test, 2–3 epochs).
- Hyperparameter tuning (LR, batch size, max_length).
- Error analysis (false positives/negatives inspection).
- Optional: wandb integration, Gradio demo.
